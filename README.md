
My implementation of ARENA's "Transformers From Scratch" along with a custom implementation of unnormalized Rotary Positional Embeddings ([RoPE](https://arxiv.org/abs/2104.09864)).

Check out the links below to see how a small transformer performs in pretraining with learned positional embeddings vs. RoPE.

[Learned positional embeddings](https://wandb.ai/lazarus42/small-transformer-with-learned-positional-embeddings/workspace?nw=nwuserlazarus42)


[RoPE](https://wandb.ai/lazarus42/small-transformer-with-rope/workspace?nw=nwuserlazarus42)

